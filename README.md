# NGrammModel
## String generation using N-gram models
### 1. Используемые файлы

model.pkl и shortmodel.pkl - файлы с обученными мной моделями. Т.к. они весят 600Мб и 500Мб, а на GitHub можно пушить максимум 100Мб-вые файлы, я загрузил эти файлы на Google Disk. Вы можете скачать их по ссылкам [model.pkl](https://drive.google.com/file/d/1fkHAmWsvGxg8y8pybZKBx-M_0JoerXrY/view?usp=sharing) и [shortmodel.pkl](https://drive.google.com/file/d/1jLxGsQApmY-oJvH0CnjAaZ9ytJ6x1HQ2/view?usp=sharing). Хотя можете обучить свои модели с помощью следующих двух программ:

Программа train.py обучает модель по текстам. Вызов программы через консоль. Параметры при вызове:

  * --input-dir − путь к директории, в которой лежит коллекция документов. Если данный аргумент не задан, считать, что тексты вводятся из консоли
  * --model − путь к файлу, в который сохраняется модель
  
Программа generate.py генерирует строку. Вызов программы через консоль. Параметры при вызове:

  * --model − путь к файлу, из которого загружается модель
  * --prefix − необязательный аргумент. Начало предложения (одно или несколько слов). Если не указано, выбираем начальное слово случайно из всех слов
  * --length − длина генерируемой последовательности
  
  В папке data хранятся пять произведений, по которым обучена модель model.pkl. Модель shortmodel.pkl обучена также по этим 5 произведениям, а также ещё по 12 произведениям из папки "Изучено только в shortmodel", вложенной в папку data. Первая модель изучает более длинные фразы (10 слов), чем вторая (5 слов).

### 2. Примеры сгенерированных текстов с помощью model.pkl:

мне будет скучно повторила она в самом деле во всяком рассказе и суждении было поразительно то как рассказчик останавливался на каждой площадке и осматривался с любопытством на площадке перед самою хозяйкиною дверью настасья светила им с нижней части лица старого князя простились ступай вдруг сказал он весело мне кажется что

я ничего не знал про

наверное то же самое будет

наверное его будущая счастливая и любящая жена приехав домой и адъютанты объявляя трофеями корпуса пленных пуки неприятельских орлов и стал

скорее всех насаливал распускал небылицу глупее которой трудно выдумать расстроивал свадьбу торговую сделку и вовсе не почитал себя вашим услугам

ты решился конечно решился и благодарю тебя от души я теперь отправляюсь на войну на величайшую войну какая только в желании сблизиться с людьми быть в фаворе у нее в лице и горничные ее знали по верным признаком того что что нибудь было не ладно между французами разъезд французов был один из тех которые были добры они не смотрели на пьера не знали его император камергер императрицы приглашал его к ее величеству эрцгерцогиня тоже желала его видеть он не знал кому отвечать и как отделаться от пришедшего в экстаз старика однажды заметив что отец в разговоре понемножку подо что то

я останусь пьер рассеянно посмотрел на наташу и что то это происходит оттого что первые приказания не соответствовали а проехать в санях можно спросил он провожавшего его до нелепого пуще всего боялся он вот уже несколько лет сорок и была она толста и жирна черноброва и был нужен да и сама по себе софья семеновна мать ты наша нежная болезная говорили эти грубые клейменые каторжные этому маленькому и худенькому созданию она улыбалась и в величественной позе взмахнув кверху рукой с своими длинными волосами в монашеской рясе на кресле подле сидела сморщенная худая старушка с кротким выражением детского лица андрюша зачем

### 3. Примеры сгенерированных текстов с помощью shortmodel.pkl:

он бросится к ногам моим ухом и слегка оттолкнув тетрадь и быстро отвернувшись принял подарок и слезы боже сохрани чтоб он первый пошел к нему поближе и сделав знак раздался одинокий выстрел сигнальной пушки и войска с простым купцом знающим одно средство выводить как можно скорее исправник приехав в свое удовольствие так уезжайте бог с вами чуть не на ухо одно и то же время похода сделал много знакомств зовов и общественных занятий князя было занятие над воротником не было ни на что не похоже на планирование оно чем происходившем что панове что скажет магницкий в то очевидно важная новость

рассчитывал тогда на ее любовь но б льшая доля расчета то не стал обдумывать он задумался совсем дурак подумал про себя принимал тяжело впрочем и в свою очередь приготовился крикнуть полк встрепенулся как будто извиняясь за нечистоту которой сердилась графиня так жил бы в своей воле и чем управлялась кутузовым с его штабом находился не по ту истину что сила войска беспрестанно оказывается неверным и написал главнокомандующему письмо в черкесском костюме верхом я и не во всем из истории как их встретившись сказали друг другу почему то раздумали когда все убрали со стола не переставая говорить и действительно я боюсь его в восхищенье мама она вдруг заплакала но когда вспомнили при этом что дал промах потом завизжал ипполит предположение обидное ложное толкование этого места ничто не помогало ее образ жизни его все отношения пьера в виде братского совета высказал ему мысль так пугала сережу что ты говоришь ты разбери ка как другой то но кутузов перебил его под руку сказал степан аркадьич с умилением пожимая руку князя андрея и я его там допытаюся какие есть ваши хитрости и с этим протянул руку левин подал ему начинать ничего было не прелесть перебила его гражданка безусловно с достоинством ответил коровьев и указал

парень и никому не отдавать будешь я правда хотел чтоб я пользовался этим обязанностям вероятно будут очень красивый с окладистой русой бородой и черными глазами но с отсутствием всякой усмешки да вы что мужики у моего отца своего рождения да

мари ты зачем ты привела их поля о глупые вопросы вскричал он это делалось с тем чтобы ничего не было а вы батюшка не только не это могло случиться степан степаныч тоже научил ежели мне нужно видеть с если он солгал вам и вы отвечать что нибудь штольцу о том чтобы провести запыленных листьях дерев бульвара в людях и в то не скажете нетерпеливо обратилась графиня к алексею александровичу ну

### 4. Описание, как работает модель

#### 4.1. Общая философия

Будем использовать уни-граммы, би-граммы, три-граммы, 4-граммы и 5-граммы. Фразы из 2-4 слов редко будут осмысленными, а для анализа 6-грамм потребуется слишком много ресурсов памяти и времени, а частота каждой отдельной 6-граммы будет мала, т.к. вероятность встретить конкретную фразу из 6 слов не так велика, таким образом мы будем долго собирать данные для 6-грамм, но эти данные нам почти не пригодятся. Однако при желании алгоритмы легко можно переписать и для более длинных N-грамм, т.к. методы для любого N будут подобными.

Для составления N-грамм создадим класс Word. Его полями будут value - само слово, counter - счётчик и next - словарь. В словаре ключами будут слова, которые могут идти после слова Word, а значениями будут опять же объекты класса Word. Таким образом мы сможем получить вложенную структуру словарей для N-грамм.

В классе NModel нашей модели создадим объект NGram класса Word c NGram.value='*', NGram.counter будет вести подсчёт, сколько всего слов мы проанализировали за всё время (для пересчёта вероятностей), а NGram.next - вложенный словарь, назовём его словарь1. В словаре1 ключами будут все изученные слова в виде объекта стандартного типа str, а значение по ключу СЛОВО1 - объект класса Word с value=СЛОВО1. counter этого объекта будет вести подсчёт этого слова в изучаемых текстах. Словарь СЛОВО1.next этого объекта ключами имеет все возможные варианты СЛОВА2 в фразе из двух слов с первым словом - СЛОВО1. Значение в СЛОВО1.next по ключу СЛОВО2 - объект Word с value=СЛОВО2 и счётчиком, сколько раз встретилась фраза "СЛОВО1 СЛОВО2". Аналогично этот объект СЛОВО2 имеет словарь СЛОВО2.next, по ключам СЛОВО3 которого располагаются объекты Word с value=СЛОВО3 и счётчиком для фразы "СЛОВО1 СЛОВО2 СЛОВО3". Так можно продолжать до любой длины фразы, но, как было сказано выше, мы в итоге остановимся на фразах из пяти слов.

#### 4.2. Обучение модели

При изучении текста берём некоторое слово СЛОВО1 из текста. Если оно есть в NGram.next, то получаем объект этого слова и увеличиваем его счётчик на 1. Если нет - то создаём в NGram.next новый объект по ключу СЛОВО1 с value=СЛОВО1, counter=1 и пустым словарём next. Далее из анализируемого текста берём следующее слово - СЛОВО2. Если оно есть в словаре СЛОВО1.next объекта СЛОВО1, полученного на предыдущем шаге, то получаем объект СЛОВО2 и увеличиваем его счётчик на 1, иначе создаём в СЛОВО1.next по ключу СЛОВО2 новый объект с value=СЛОВО2, counter=1 и пустым словарём next. Аналогично рассматриваем СЛОВО3, СЛОВО4, СЛОВО5. После этого смещаемся в тексте на одно слово и теперь СЛОВО2 стало СЛОВОМ1, опять для него ищем совпадения в NGram.next и тд для всех возможных вариантов СЛОВО1 в изучаемом тексте.

#### 4.3. Генерация текста

При генерации текста, используя NGram.next.keys(), зная из NGram.counter общее число проанализированных слов и имея счётчики для каждого слова, рассчитаем частоту каждого слова и c учётом этой частоты, используя np.random.choice(), выберем СЛОВО1 из NGram.next.values(). Далее из NGram.next получим объект СЛОВО1 по ключу СЛОВО1. С учётом частот фраз "СЛОВО1 СЛОВО2" для фиксированного СЛОВА1 и всех возможных вариантов продолжения СЛОВО2 опять np.random.choice() выберем следующее слово.
Аналогично получим слова СЛОВО3, СЛОВО4, СЛОВО5. После этого делаем СЛОВО5 первым словом и ещё раз ищем подходящую пятёрку слов и так пока не получим фразу нужной длины.

### 5. Дополнения

Чтобы повторно не учить одни и те же файлы, создадим поле-список alreadyKnown с путями уже изученных файлов.

PS после создания модели меня всё же не устроила связность текста при ограничении 5-граммной моделью, поэтому путём экспериментов я пришёл к 10-граммной модели. Главная проблема - запись объекта model в файл через pickle.dump() и чтение модели из файла с помощью pickle.load(). Анализ текстовых файлов происходит достаточно быстро, однако так как в модели используются многоуровневые словари, то их сохранение в файл требует много памяти. Поэтому если мы изучим слишком много текстов или будем запоминать слишком длинные последовательности, то памяти не хватает, программа долго будет пытаться записать модель в файл, пока не выдаст ошибку MemoryError. Поэтому есть два варианта: либо изучать много разных текстов, но использовать только короткие префиксы, либо изучить мало текстов, но использовать более длинные префиксы. В первом случае мы получим достаточно разнообразные генерируемые строки, но они будут иметь мало смысла. Во втором же случае у нас будут более однотипные длинные строки, но зато они будут более осмысленными. Поиграв с этими двумя параметрами, я решил обучить модель по 5 популярным классическим произведениям русской литературы (Война и мир, Отцы и дети, Герои нашего времени, Мёртвые души, Преступление и наказание) и использовал при этом 10-граммную модель. Эта модель хранится в файле model.pkl. Также обучена модель shortmodel.pkl: она использует 5-граммы, но уже из 17 произведений, и она сохраняется и загружается быстрее.

